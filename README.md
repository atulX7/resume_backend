#  ğŸ¤– Resume Evaluator & Mock Interview API

A scalable FastAPI backend application for resume evaluation, mock interviews, scoring, and AI-driven feedback â€” using OpenAI, AWS S3, SQS, and PostgreSQL.

---

## ğŸš€ Features

- ğŸ” Resume parsing and AI-based scoring
- ğŸ¤ Mock interview with audio recording and transcription
- ğŸ¤– GPT-powered interview feedback and improvement suggestions
- â˜ï¸ AWS integrations: S3 (resume/audio), SQS (job queue), Transcribe (audio-to-text)
- ğŸ§  JD alignment + keyword analysis + ATS-friendly evaluation

---

## ğŸ“¦ Tech Stack

- **Backend**: FastAPI + Pydantic
- **Task Queue**: AWS SQS
- **AI Integration**: OpenAI (GPT-4o)
- **Cloud Services**: AWS S3, Transcribe
- **Auth & Roles**: JWT & custom scopes
- **CI/CD**: GitHub Actions

---
## â˜ï¸ AWS Integration

This application interacts with AWS for several services. You must ensure the EC2 instance or user running the backend has the correct **IAM Role** attached.

---

### âœ… Required IAM Policies

Attach the following managed policies to your IAM Role (or IAM User, if using access keys):

- `AmazonS3FullAccess` â€“ For uploading/downloading resumes and interview artifacts
- `AmazonSQSFullAccess` â€“ For sending/receiving interview processing jobs via SQS
- `AmazonTranscribeFullAccess` â€“ For converting audio to text during mock interviews

---
### ğŸ—‚ï¸ S3 â€œtmp/â€ Folder & Lifecycle Cleanup

We use a **temporary** S3 prefix (`tmp/`) to stage uploaded resumes before a session is confirmed. To avoid orphaned files and unnecessary storage costs, add a lifecycle rule:

1. **Prefix:** `tmp/`
2. **Action:** Expire (Permanently delete) objects
3. **Age:** 24 hours (or your desired TTL)

```yaml
# Example S3 Lifecycle configuration (in JSON)
{
  "Rules": [
    {
      "ID": "TmpFolderAutoExpire",
      "Prefix": "tmp/",
      "Status": "Enabled",
      "Expiration": { "Days": 1 },
      "NoncurrentVersionExpiration": { "NoncurrentDays": 1 }
    }
  ]
}
```

---

### ğŸ“¬ AWS SQS Queue (Mock Interview Processing)

An SQS queue is used to offload intensive tasks like audio transcription and AI evaluation.

**Steps:**

1. Create an SQS queue (e.g., `mock-interview-queue`)
2. Copy the SQS URL (e.g.,  
   `https://sqs.ap-south-1.amazonaws.com/123456789012/mock-interview-queue`)
3. Add it to your `.env` file:

```env
SQS_MOCK_INTERVIEW_QUEUE_URL=https://sqs.ap-south-1.amazonaws.com/123456789012/mock-interview-queue
```

## ğŸ› ï¸ Local Development Setup

### âœ… Prerequisites

Make sure the following tools are installed on your system:

- Python `3.11`
- [Poetry](https://python-poetry.org/docs/)
- PostgreSQL (locally or cloud-managed)
- AWS credentials (for S3 & SQS access)
- OpenAI API key (for resume evaluations)

---

### ğŸ› ï¸ Step 1: Clone & Install

```bash
git clone https://github.com/atulX7/resume_backend.git
cd resume_backend

# Install all dependencies
poetry install
```


### âš™ï¸ Step 2: Environment Variables

Create a .env file in the project root:
```bash
DATABASE_URL=postgresql+psycopg2://username:password@localhost:5432/yourdb
AWS_ACCESS_KEY=your_aws_access_key
AWS_SECRET_KEY=your_aws_secret_key
AWS_REGION_NAME=ap-south-1
S3_BUCKET_NAME=your_s3_bucket

OPENAI_API_KEY=your_openai_api_key

SMTP_SERVER=smtp.example.com
SMTP_PORT=587
SMTP_USERNAME=you@example.com
SMTP_PASSWORD=your_password

ALLOW_ORIGINS=http://localhost:3000
MOCK_DATA=True
SQS_MOCK_INTERVIEW_QUEUE_URL=https://sqs.ap-south-1.amazonaws.com/xxx/mock-interview-queue
SEED_DB=True
```

### ğŸ§ª Step 3: Database Migrations
```bash
# Run migrations using Alembic
poetry run alembic upgrade head
```

### ğŸš€ Step 4: Run the FastAPI App & SQS worker
```bash
# Run migrations using Alembic
poetry run uvicorn app.main:app --reload

# Run worker manually
poetry run python sqs_worker.py
```
Visit the interactive API docs at: http://localhost:8000/docs

---

## ğŸ§° Project Structure
```bash
resume_backend/
â”œâ”€â”€ .github/                 # GitHub workflows and actions
â”œâ”€â”€ alembic/                 # Alembic migration files
â”œâ”€â”€ app/                     # Main FastAPI application
â”‚   â”œâ”€â”€ api/                 # Route definitions
â”‚   â”œâ”€â”€ core/                # Settings, logging, and configuration
â”‚   â”œâ”€â”€ database/            # Database models, sessions, and queries
â”‚   â”œâ”€â”€ middleware/          # Custom middlewares
â”‚   â”œâ”€â”€ models/              # ORM model definitions
â”‚   â”œâ”€â”€ schemas/             # Pydantic schema models
â”‚   â”œâ”€â”€ services/            # Core business logic
â”‚   â”œâ”€â”€ utils/               # Utility functions (OpenAI, AWS, etc.)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py              # FastAPI entrypoint
â”œâ”€â”€ logs/                    # Centralized log directory
â”œâ”€â”€ .env                     # Local environment configuration
â”œâ”€â”€ alembic.ini              # Alembic config
â”œâ”€â”€ poetry.lock              # Locked package versions
â”œâ”€â”€ pyproject.toml           # Project & dependency config
â”œâ”€â”€ README.md                # You're reading it!
â””â”€â”€ sqs_worker.py            # Background worker for SQS queue
```

---

## ğŸ›¡ï¸ Deployment
This project uses GitHub Actions to deploy automatically to an EC2 instance. See .github/workflows/deploy.yml for the full provisioning & deployment flow:

âœ… Git-based EC2 pull & reset

âœ… Python & Poetry install via pyenv

âœ… .env injection from GitHub secrets

âœ… Alembic DB migrations

âœ… FastAPI server start + systemd SQS worker setup


---

## ğŸ§  Features

ğŸ§¾ Resume Parsing & Scoring via OpenAI

ğŸ§  Mock Interview Q&A + Feedback System

ğŸ“¤ Upload/Download Resumes via S3

âš™ï¸ SQS Worker Integration for Async Processing

âœ‰ï¸ Email Feedback Delivery


---

## ğŸ§¹ Code Quality
```bash
# Auto-format imports & lint
ruff check . --fix
```
---

## ğŸ“« Contact

For queries, reach out to [@atulx7](https://github.com/atulx7) on GitHub.


---

ğŸ§  Built with â¤ï¸ using FastAPI, AWS, OpenAI, and a touch of MLOps magic.
